{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05868709-fa4f-41f8-bd5f-9212280aee62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TAIWAN DATASET CHECK ---\n",
      "Taiwan X_train shape: (14999, 26)\n",
      "Taiwan X_test shape: (9000, 26)\n",
      "Taiwan X_val shape: (6001, 26)\n",
      "Taiwan A_train unique values: [0 1]\n",
      "\n",
      "--- GERMAN DATASET CHECK ---\n",
      "German X_train shape: (499, 24)\n",
      "German X_test shape: (300, 24)\n",
      "Taiwan X_val shape: (201, 24)\n",
      "German A_train unique values: [0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, clone\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, confusion_matrix\n",
    "from fairlearn.metrics import demographic_parity_difference, equalized_odds_difference, MetricFrame # for predictive_parity_odds\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "'''\n",
    "============================================================\n",
    " Functions for Data Pre-Processing\n",
    "============================================================\n",
    "'''\n",
    "\n",
    "DATA_DIR = \"\" # todo: modify this path to match your folder path\n",
    "\n",
    "def load_taiwan_dataset(test_size=0.3, val_size=0.2, random_state=42, sensitive=\"sex\"):\n",
    "    '''\n",
    "    Load the Taiwan dataset (UCI Credit Card).\n",
    "    sensitive: str, \"sex\" or \"age\"\n",
    "    '''\n",
    "    path = os.path.join(DATA_DIR, \"UCI_Credit_Card.csv\")\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.drop(columns=[\"ID\"])\n",
    "\n",
    "    # Target\n",
    "    y = df[\"default.payment.next.month\"].values\n",
    "\n",
    "    # Sensitive attributes\n",
    "    df[\"SEX_BIN\"] = df[\"SEX\"].map({1: 1, 2: 0})\n",
    "    df[\"AGE_NUM\"] = df[\"AGE\"]\n",
    "    df[\"AGE_GROUP\"] = pd.cut(\n",
    "        df[\"AGE\"], bins=[0, 25, 35, 45, 60, 120], labels=[0, 1, 2, 3, 4]\n",
    "    ).astype(int)\n",
    "\n",
    "    if sensitive.lower() == \"sex\":\n",
    "        A = df[\"SEX_BIN\"].values\n",
    "    else:\n",
    "        A = df[\"AGE_GROUP\"].values\n",
    "\n",
    "    df_features = df.drop(columns=[\"default.payment.next.month\"])\n",
    "    X = pd.get_dummies(df_features, drop_first=True)\n",
    "\n",
    "    # train/validate/test split\n",
    "    X_temp, X_test, y_temp, y_test, A_temp, A_test = train_test_split(\n",
    "        X, y, A,\n",
    "        test_size=test_size,\n",
    "        stratify=y,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    val_rel = val_size / (1.0 - test_size)\n",
    "    X_train, X_val, y_train, y_val, A_train, A_val = train_test_split(\n",
    "        X_temp, y_temp, A_temp,\n",
    "        test_size=val_rel,\n",
    "        stratify=y_temp,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    num_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "    X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
    "    X_val[num_cols] = scaler.transform(X_val[num_cols])\n",
    "    X_test[num_cols] = scaler.transform(X_test[num_cols])\n",
    "\n",
    "    return (\n",
    "        X_train, X_val, X_test,\n",
    "        y_train, y_val, y_test,\n",
    "        A_train, A_val, A_test,\n",
    "        scaler,\n",
    "    )\n",
    "\n",
    "def load_german_dataset(test_size=0.3, val_size=0.2, random_state=42, sensitive=\"sex\"):\n",
    "    '''\n",
    "    Load the German dataset (German Credit).\n",
    "    sensitive: str, \"sex\" or \"age\"\n",
    "    '''\n",
    "    path = os.path.join(DATA_DIR, \"Processed_German_Credit.csv\")\n",
    "    df = pd.read_csv(path)\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "    # Target\n",
    "    y = df[\"credit\"].values\n",
    "\n",
    "    df[\"sex_bin\"] = df[\"sex\"].map({\"male\": 1, \"female\": 0})\n",
    "\n",
    "    df[\"age_num\"] = df[\"age\"]\n",
    "    df[\"age_group\"] = pd.cut(\n",
    "        df[\"age\"], bins=[0, 25, 35, 45, 60, 120], labels=[0, 1, 2, 3, 4]\n",
    "    ).astype(int)\n",
    "\n",
    "    if sensitive.lower() == \"sex\":\n",
    "        A = df[\"sex_bin\"].values\n",
    "    else:\n",
    "        A = df[\"age_group\"].values\n",
    "\n",
    "    # drop_cols = drop_tgt + [\"sex\", \"age\"]\n",
    "    # drop_cols = [\"sex\", \"age\"]\n",
    "    # df_features = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "\n",
    "    df_features = df.drop(columns=[\"credit\"])\n",
    "    X = pd.get_dummies(df_features, drop_first=True)\n",
    "\n",
    "    # train/val/test split\n",
    "    X_temp, X_test, y_temp, y_test, A_temp, A_test = train_test_split(\n",
    "        X, y, A,\n",
    "        test_size=test_size,\n",
    "        stratify=y,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    val_rel = val_size / (1.0 - test_size)\n",
    "    X_train, X_val, y_train, y_val, A_train, A_val = train_test_split(\n",
    "        X_temp, y_temp, A_temp,\n",
    "        test_size=val_rel,\n",
    "        stratify=y_temp,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    num_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "    X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
    "    X_val[num_cols] = scaler.transform(X_val[num_cols])\n",
    "    X_test[num_cols] = scaler.transform(X_test[num_cols])\n",
    "\n",
    "    return (\n",
    "        X_train, X_val, X_test,\n",
    "        y_train, y_val, y_test,\n",
    "        A_train, A_val, A_test,\n",
    "        scaler,\n",
    "    )\n",
    "\n",
    "def load_dataset(name, sensitive=\"sex\", **kwargs):\n",
    "    '''\n",
    "    Call either load_taiwan_dataset() or load_german_dataset() functions to load the dataset.\n",
    "    '''\n",
    "    if name.lower() == 'taiwan':\n",
    "        return load_taiwan_dataset(sensitive=sensitive, **kwargs)\n",
    "    elif name.lower() == 'german':\n",
    "        return load_german_dataset(sensitive=sensitive, **kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {name}\")\n",
    "\n",
    "print(\"\\n--- TAIWAN DATASET CHECK ---\")\n",
    "X_train_t, X_val_t, X_test_t, y_train_t, y_val_t, y_test_t, A_train_t, A_val_t, A_test_t, _ = load_taiwan_dataset()\n",
    "print(f\"Taiwan X_train shape: {X_train_t.shape}\")\n",
    "print(f\"Taiwan X_test shape: {X_test_t.shape}\")\n",
    "print(f\"Taiwan X_val shape: {X_val_t.shape}\")\n",
    "print(f\"Taiwan A_train unique values: {np.unique(A_train_t)}\")\n",
    "\n",
    "print(\"\\n--- GERMAN DATASET CHECK ---\")\n",
    "X_train_g, X_val_g, X_test_g, y_train_g, y_val_g, y_test_g, A_train_g, A_val_g, A_test_g, _ = load_german_dataset(sensitive=\"age\")\n",
    "print(f\"German X_train shape: {X_train_g.shape}\")\n",
    "print(f\"German X_test shape: {X_test_g.shape}\")\n",
    "print(f\"Taiwan X_val shape: {X_val_g.shape}\")\n",
    "print(f\"German A_train unique values: {np.unique(A_train_g)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "============================================================\n",
    " Functions for Computing Evaluation Metrics\n",
    "============================================================\n",
    "'''\n",
    "def compute_metrics(y_true, y_pred, A):\n",
    "    '''\n",
    "    computes evaluation metrics (acc, prec, rec, f1, mcc), confusion matrix, and \n",
    "    fairness metrics (absolute differences of dp, eo, pp).\n",
    "    '''\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "    metric_frame = MetricFrame(\n",
    "        metrics={\"precision\": precision_score},\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        sensitive_features=A\n",
    "    )\n",
    "    group_precision = metric_frame.by_group['precision']\n",
    "    \n",
    "    dp_diff = demographic_parity_difference(y_true=y_true, y_pred=y_pred, sensitive_features=A)\n",
    "    eo_diff = equalized_odds_difference(y_true=y_true, y_pred=y_pred, sensitive_features=A)\n",
    "    pp_diff = abs(group_precision.max() - group_precision.min())\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"mcc\": mcc,\n",
    "        \"dp_diff\": dp_diff,\n",
    "        \"eo_diff\": eo_diff,\n",
    "        \"pp_diff\": pp_diff,\n",
    "        \"confusion_matrix\": confusion_matrix(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "def predictive_parity_difference(y_true, y_pred, sensitive_features):\n",
    "    groups = np.unique(sensitive_features)\n",
    "    ppv = {}\n",
    "\n",
    "    for g in groups:\n",
    "        mask = sensitive_features == g\n",
    "        # among predicted positives, how many are correct?\n",
    "        preds_pos = (y_pred[mask] == 1)\n",
    "        \n",
    "        if preds_pos.sum() == 0:\n",
    "            ppv[g] = 0   # or np.nan, depending on your preference\n",
    "        else:\n",
    "            ppv[g] = (y_true[mask][preds_pos] == 1).mean()\n",
    "\n",
    "    # max difference between any two groups\n",
    "    gaps = []\n",
    "    for g1 in groups:\n",
    "        for g2 in groups:\n",
    "            gaps.append(abs(ppv[g1] - ppv[g2]))\n",
    "\n",
    "    return max(gaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "============================================================\n",
    " Functions for Plotting Figures\n",
    "============================================================\n",
    "'''\n",
    "\n",
    "# modified plot_tradeoff\n",
    "def plot_pareto_tradeoff(val_results, title_suffix=\"\"):\n",
    "    '''\n",
    "    Plot the pareto-style trade-off for accuracy vs all fairness metrics\n",
    "    '''\n",
    "    lambdas = [res[\"lambda\"] for res in val_results]\n",
    "    accs = [res[\"accuracy\"] for res in val_results]\n",
    "    dps = [res[\"dp\"] for res in val_results]\n",
    "    eos = [res[\"eo\"] for res in val_results]\n",
    "    pps = [res[\"pp\"] for res in val_results]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    plt.plot(dps, accs, marker=\"o\", label=\"DP (ΔP)\")\n",
    "    plt.plot(eos, accs, marker=\"s\", label=\"EO (ΔTPR + ΔFPR)\")\n",
    "    plt.plot(pps, accs, marker=\"^\", label=\"PP (ΔPPV)\")\n",
    "\n",
    "    for dp, eo, pp, acc, lam in zip(dps, eos, pps, accs, lambdas):\n",
    "        plt.annotate(f\"{lam}\", (dp, acc), textcoords=\"offset points\", xytext=(5, 5), fontsize=8)\n",
    "        plt.annotate(f\"{lam}\", (eo, acc), textcoords=\"offset points\", xytext=(5, 5), fontsize=8)\n",
    "        plt.annotate(f\"{lam}\", (pp, acc), textcoords=\"offset points\", xytext=(5, 5), fontsize=8)\n",
    "\n",
    "    plt.xlabel(\"Fairness Metric Value (lower = fairer)\")\n",
    "    plt.ylabel(\"Validation Accuracy\")\n",
    "    plt.title(f\"Accuracy vs Fairness Tradeoff {title_suffix}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_lambda_sensitivity(results, fairness_key=\"dp_diff\", title=\"Lambda Sensitivity\"):\n",
    "    lambdas = [r[\"lambda\"] for r in results]\n",
    "    acc = [r[\"accuracy\"] for r in results]\n",
    "    fairness = [r[fairness_key] for r in results]\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(7,5))\n",
    "    \n",
    "    ax1.plot(lambdas, acc, marker=\"o\", label=\"Accuracy\")\n",
    "    ax1.set_xlabel(\"Lambda\")\n",
    "    ax1.set_ylabel(\"Accuracy\")\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(lambdas, fairness, marker=\"s\", color=\"red\", label=fairness_key.upper())\n",
    "    ax2.set_ylabel(f\"{fairness_key.upper()} Gap\", color=\"red\")\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def plot_training_dynamics(round_metrics, fairness_key=\"dp_diff\", title=\"Boosting Dynamics\"):\n",
    "    rounds = [m[\"round\"] for m in round_metrics]\n",
    "    accs = [m[\"accuracy\"] for m in round_metrics]\n",
    "    fairness_vals = [m[fairness_key] for m in round_metrics]\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "\n",
    "    plt.plot(rounds, accs, marker=\"o\", label=\"Accuracy\")\n",
    "    plt.plot(rounds, fairness_vals, marker=\"s\", label=fairness_key)\n",
    "\n",
    "    plt.xlabel(\"Boosting Round\")\n",
    "    plt.ylabel(\"Metric Value\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "''' \n",
    "============================================================\n",
    " AdaBoost implementation w/ Perceptron\n",
    "============================================================\n",
    "'''\n",
    "class AdaBoostClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, \n",
    "                 base_estimator=None, \n",
    "                 n_estimators=50, \n",
    "                 learning_rate=1.0,\n",
    "                 fairness_lambda=0.0,\n",
    "                 fairness_tolerance=0.5,\n",
    "                 random_state=None):\n",
    "\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.random_state = random_state\n",
    "        self.fairness_lambda = fairness_lambda\n",
    "        self.fairness_tolerance = fairness_tolerance\n",
    "\n",
    "        self.estimators_ = []\n",
    "        self.estimator_weights_ = []\n",
    "        self.classes_ = None\n",
    "\n",
    "        self.round_metrics = []   # store metrics per round\n",
    "\n",
    "    def check_binary(self, y):\n",
    "        y = np.asarray(y)\n",
    "        classes = np.unique(y)\n",
    "        if len(classes) != 2:\n",
    "            raise ValueError(\"Binary classification only.\")\n",
    "\n",
    "        self.classes_ = classes\n",
    "        return (y == classes[1]).astype(int)\n",
    "\n",
    "    def fit(self, X, y, A=None):\n",
    "        '''\n",
    "        fit with fairness modification using demographic parity gap.\n",
    "        A is the sensitive attribute (0/1).\n",
    "        '''\n",
    "        if self.base_estimator is None:\n",
    "            raise ValueError(\"base_estimator must be provided.\")\n",
    "\n",
    "        if A is None:\n",
    "            raise ValueError(\"Sensitive attribute A must be provided for fairness-aware training.\")\n",
    "\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        A = np.asarray(A)\n",
    "\n",
    "        y01 = self.check_binary(y)\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        sample_weight = np.ones(n_samples) / n_samples\n",
    "\n",
    "        rng = np.random.RandomState(self.random_state) # random states\n",
    "\n",
    "        self.estimators_ = []\n",
    "        self.estimator_weights_ = []\n",
    "\n",
    "        for t in range(self.n_estimators):\n",
    "            # est = clone(self.base_estimator)\n",
    "\n",
    "            # modified @ 12/11\n",
    "            # choose base learner in round-robin fashion\n",
    "            est_idx = t % len(self.base_estimator)\n",
    "            est = clone(self.base_estimator[est_idx])\n",
    "            # print(f\"Round {t}: Using estimator {type(est).__name__}\")\n",
    "\n",
    "            # standard sample weight\n",
    "            # est.fit(X, y01, sample_weight=sample_weight)\n",
    "            # replace with :\n",
    "            # weighted resampling (because SVM and MLP do not accept sample_weights)\n",
    "            idx = rng.choice(\n",
    "                np.arange(n_samples),\n",
    "                size=n_samples,\n",
    "                replace=True,\n",
    "                p=sample_weight\n",
    "            )\n",
    "            est.fit(X[idx], y01[idx])\n",
    "\n",
    "            y_pred = est.predict(X).astype(int)\n",
    "            incorrect = (y_pred != y01).astype(float)\n",
    "\n",
    "            err = np.dot(sample_weight, incorrect) / sample_weight.sum()\n",
    "            err = np.clip(err, 1e-10, 1 - 1e-10)\n",
    "            acc = accuracy_score(y, y_pred)\n",
    "\n",
    "            # modified @ 12/11\n",
    "            # fairness penalty and learning rates\n",
    "            lambda_dp = 0.1\n",
    "            lambda_eo = 0.3\n",
    "            lambda_pp = 0.5\n",
    "            \n",
    "            dp_gap = demographic_parity_difference(y_true=y01, y_pred=y_pred, sensitive_features=A)\n",
    "            eo_gap = equalized_odds_difference(y_true=y01, y_pred=y_pred, sensitive_features=A)\n",
    "            pp_gap = predictive_parity_difference(y01, y_pred, A)\n",
    "\n",
    "            dp = np.clip(dp_gap, 0, 1)\n",
    "            eo = np.clip(eo_gap, 0, 1)\n",
    "            pp = np.clip(pp_gap, 0, 1)\n",
    "            \n",
    "            # # if dp_gap > dp_tolerance, then we add a penalty\n",
    "            # fairness_penalty = self.fairness_lambda * max(0.0, dp_gap - self.dp_tolerance)\n",
    "\n",
    "            if self.fairness_lambda >= self.fairness_tolerance:\n",
    "                # modified @ 12/11\n",
    "                # compute penalty equation, and classifier weight based on fairness penalty)\n",
    "                fairness_penalty = (lambda_dp * dp) + (lambda_eo * eo) + (lambda_pp * pp)\n",
    "                lambda_t = np.exp(-self.fairness_lambda * fairness_penalty)\n",
    "            else: # fairness OFF (baseline AdaBoost)\n",
    "                fairness_penalty = 0.0\n",
    "                lambda_t = 1.0\n",
    "\n",
    "            # # combined error\n",
    "            # combined_err = err + fairness_penalty\n",
    "            # combined_err = np.clip(combined_err, 1e-10, 1 - 1e-10)\n",
    "            # # modified @ 12/07 || end\n",
    "\n",
    "            # # alpha = self.learning_rate * 0.5 * np.log((1 - err) / err)\n",
    "            # alpha = 0.5 * np.log((1 - combined_err) / combined_err) * self.learning_rate\n",
    "\n",
    "            alpha = 0.5 * np.log((1 - err) / err) * self.learning_rate\n",
    "            if alpha <= 0:\n",
    "                break\n",
    "\n",
    "            # sample_weight *= np.exp(alpha * incorrect.astype(float))\n",
    "            # sample_weight /= sample_weight.sum()\n",
    "            sample_weight *= np.exp(-alpha * (2 * y01 - 1) * (2 * y_pred - 1))\n",
    "            sample_weight *= np.exp(-self.fairness_lambda * fairness_penalty)\n",
    "            sample_weight /= sample_weight.sum()\n",
    "\n",
    "            self.estimators_.append(est)\n",
    "            self.estimator_weights_.append(alpha * lambda_t)\n",
    "            self.round_metrics.append({\n",
    "                \"round\": t + 1,\n",
    "                \"accuracy\": acc,\n",
    "                \"dp_diff\": dp,\n",
    "                \"eo_diff\": eo,\n",
    "                \"pp_diff\": pp\n",
    "            })\n",
    "\n",
    "        self.estimator_weights_ = np.array(self.estimator_weights_)\n",
    "        return self\n",
    "\n",
    "    def scores(self, X):\n",
    "        X = np.asarray(X)\n",
    "        scores = np.zeros(X.shape[0])\n",
    "\n",
    "        for alpha, est in zip(self.estimator_weights_, self.estimators_):\n",
    "            pred = est.predict(X).astype(int)\n",
    "            pred_pm1 = 2 * pred - 1\n",
    "            scores += alpha * pred_pm1\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def predict(self, X):\n",
    "        scores = self.scores(X)\n",
    "        y01 = (scores >= 0).astype(int)\n",
    "        return np.where(y01 == 1, self.classes_[1], self.classes_[0])\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        scores = self.scores(X)\n",
    "        p_pos = 1 / (1 + np.exp(-2 * scores))\n",
    "        return np.vstack([1 - p_pos, p_pos]).T\n",
    "        \n",
    "# run training and testing sets; validate the outputs\n",
    "def run_experiment(\n",
    "    dataset_name,\n",
    "    sensitive=\"sex\",\n",
    "    lambdas=(0.0, 0.1, 0.3, 0.5, 1.0),\n",
    "    n_estimators=50,\n",
    "    fairness_tolerance=0.05, # for fairness modifications\n",
    "):\n",
    "    '''\n",
    "    runs the experiment for a given dataset and sensitive attribute.\n",
    "    '''\n",
    "    (\n",
    "        X_train, X_val, X_test,\n",
    "        y_train, y_val, y_test,\n",
    "        A_train, A_val, A_test, # this is for the fairness modification\n",
    "        scaler,\n",
    "    ) = load_dataset(dataset_name, sensitive=sensitive)\n",
    "\n",
    "    print(f\"Dataset: {dataset_name}, sensitive: {sensitive}\")\n",
    "    print(\"Train shape:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n",
    "\n",
    "    # use a Perceptron as the base learner instead of a decision tree stump\n",
    "    # base = Perceptron(\n",
    "    #     max_iter=1000,\n",
    "    #     eta0=0.01,\n",
    "    #     random_state=0,\n",
    "    #     tol=1e-3,\n",
    "    #     fit_intercept=True\n",
    "    # )\n",
    "\n",
    "    base = [\n",
    "        Perceptron(max_iter=1000, eta0=0.01, random_state=0, tol=1e-3, fit_intercept=True),\n",
    "        SVC(probability=True, kernel='rbf', C=1.0),\n",
    "        MLPClassifier(\n",
    "            hidden_layer_sizes=(128, 32),\n",
    "            max_iter=1000,\n",
    "            early_stopping=True,\n",
    "            n_iter_no_change=20,\n",
    "            validation_fraction=0.1,\n",
    "            random_state=42,\n",
    "            learning_rate_init=0.001,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    val_results = []\n",
    "\n",
    "    for lam in lambdas:\n",
    "        model = AdaBoostClassifier(\n",
    "            base_estimator=base,\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=1.0,\n",
    "            fairness_lambda=lam, # for fairness modifications\n",
    "            fairness_tolerance=fairness_tolerance # for fairness modifications\n",
    "        )\n",
    "        # fairness-aware fit uses A_train\n",
    "        model.fit(X_train, y_train, A_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        metrics = compute_metrics(y_val, y_val_pred, A_val)\n",
    "        metrics[\"lambda\"] = lam\n",
    "        # val_results.append(metrics)\n",
    "        val_res.append({\n",
    "            \"metrics\": metrics,\n",
    "            \"lambda\": fairness_lambda,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"dp\": dp,\n",
    "            \"eo\": eo,\n",
    "            \"pp\": pp,\n",
    "            \"round_metrics\": model.round_metrics\n",
    "        })\n",
    "\n",
    "\n",
    "        print(\n",
    "            f\"lambda={lam:.2f} | \"\n",
    "            f\"Acc={metrics['accuracy']:.3f}, F1={metrics['f1']:.3f}, \"\n",
    "            f\"MCC={metrics['mcc']:.3f}, DP={metrics['dp_diff']:.3f}, \"\n",
    "    \t    f\"EO={metrics['eo_diff']:.3f}, PP={metrics['pp_diff']:.3f}\"\n",
    "        )\n",
    "\n",
    "    best_idx = np.argmax([m[\"f1\"] for m in val_results])\n",
    "    best_lam = val_results[best_idx][\"lambda\"]\n",
    "    print(\"\\nBest lambda on validation (by F1):\", best_lam)\n",
    "\n",
    "    X_tr_full = pd.concat([X_train, X_val], axis=0)\n",
    "    y_tr_full = np.concatenate([y_train, y_val])\n",
    "    A_tr_full = np.concatenate([A_train, A_val])\n",
    "\n",
    "    best_model = AdaBoostClassifier(\n",
    "        base_estimator=base,\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=1.0,\n",
    "        fairness_lambda=best_lam,\n",
    "        fairness_tolerance=fairness_tolerance,\n",
    "    )\n",
    "    best_model.fit(X_tr_full, y_tr_full, A_tr_full)\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "\n",
    "    test_metrics = compute_metrics(y_test, y_test_pred, A_test)\n",
    "    print(\"\\n=== Test metrics with best lambda ===\")\n",
    "    for k, v in test_metrics.items():\n",
    "        if k == \"confusion_matrix\":\n",
    "            print(k, \"=\\n\", v)\n",
    "        else:\n",
    "            print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
    "\n",
    "    return val_results, test_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "''' \n",
    "==================================================================\n",
    " Run AdaBoost experiments (Round-Robin Heterogeneous)\n",
    "==================================================================\n",
    "'''\n",
    "# Taiwan dataset fairness on sex\n",
    "val_res_taiwan, test_taiwan = run_experiment(\n",
    "    dataset_name=\"taiwan\",\n",
    "    sensitive=\"sex\",\n",
    "    lambdas=(0.0, 0.1, 0.3, 0.5, 0.8, 1.0),\n",
    "    n_estimators=40,\n",
    "    fairness_tolerance=0.5, # for fairness modifications\n",
    ")\n",
    "plot_pareto(val_res_taiwan, title_suffix=\"(Taiwan, Sex)\")\n",
    "plot_training_dynamics(result[\"round_metrics\"], fairness_key=\"dp_diff\", title=\"Training Dynamics (λ=0.5)\")\n",
    "plot_training_dynamics(result[\"round_metrics\"], fairness_key=\"eo_diff\", title=\"Training Dynamics (λ=0.5)\")\n",
    "plot_training_dynamics(result[\"round_metrics\"], fairness_key=\"pp_diff\", title=\"Training Dynamics (λ=0.5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taiwan dataset fairness on age\n",
    "val_res_taiwan, test_taiwan = run_experiment(\n",
    "    dataset_name=\"taiwan\",\n",
    "    sensitive=\"age\",\n",
    "    lambdas=(0.0, 0.1, 0.3, 0.5, 0.8, 1.0),\n",
    "    n_estimators=40,\n",
    "    fairness_tolerance=0.5, # for fairness modifications\n",
    ")\n",
    "plot_pareto(val_res_taiwan, title_suffix=\"(Taiwan, Age)\")\n",
    "plot_training_dynamics(result[\"round_metrics\"], fairness_key=\"dp_diff\", title=\"Training Dynamics (λ=0.5)\")\n",
    "plot_training_dynamics(result[\"round_metrics\"], fairness_key=\"eo_diff\", title=\"Training Dynamics (λ=0.5)\")\n",
    "plot_training_dynamics(result[\"round_metrics\"], fairness_key=\"pp_diff\", title=\"Training Dynamics (λ=0.5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# German dataset fairness on sex\n",
    "val_res_taiwan, test_taiwan = run_experiment(\n",
    "    dataset_name=\"taiwan\",\n",
    "    sensitive=\"sex\",\n",
    "    lambdas=(0.0, 0.1, 0.3, 0.5, 0.8, 1.0),\n",
    "    n_estimators=40,\n",
    "    fairness_tolerance=0.5, # for fairness modifications\n",
    ")\n",
    "plot_pareto(val_res_taiwan, title_suffix=\"(Taiwan, Sex)\")\n",
    "plot_training_dynamics(result[\"round_metrics\"], fairness_key=\"dp_diff\", title=\"Training Dynamics (λ=0.5)\")\n",
    "plot_training_dynamics(result[\"round_metrics\"], fairness_key=\"eo_diff\", title=\"Training Dynamics (λ=0.5)\")\n",
    "plot_training_dynamics(result[\"round_metrics\"], fairness_key=\"pp_diff\", title=\"Training Dynamics (λ=0.5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# German dataset fairness on age\n",
    "val_res_taiwan, test_taiwan = run_experiment(\n",
    "    dataset_name=\"taiwan\",\n",
    "    sensitive=\"age\",\n",
    "    lambdas=(0.0, 0.1, 0.3, 0.5, 0.8, 1.0),\n",
    "    n_estimators=40,\n",
    "    fairness_tolerance=0.5, # for fairness modifications\n",
    ")\n",
    "plot_pareto(val_res_taiwan, title_suffix=\"(Taiwan, Age)\")\n",
    "plot_training_dynamics(result[\"round_metrics\"], fairness_key=\"dp_diff\", title=\"Training Dynamics (λ=0.5)\")\n",
    "plot_training_dynamics(result[\"round_metrics\"], fairness_key=\"eo_diff\", title=\"Training Dynamics (λ=0.5)\")\n",
    "plot_training_dynamics(result[\"round_metrics\"], fairness_key=\"pp_diff\", title=\"Training Dynamics (λ=0.5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# '''\n",
    "# ============================================================\n",
    "# Baseline ANN model using MLPClassifier (2 hidden layers)\n",
    "# ============================================================\n",
    "# '''\n",
    "\n",
    "# def run_mlp_baseline(\n",
    "#     dataset_name,\n",
    "#     sensitive=\"sex\",\n",
    "#     hidden_layer_sizes=(64, 32),\n",
    "#     max_iter=500,\n",
    "#     random_state=0,\n",
    "# ):\n",
    "#     # train a standard (non-boosted) MLPClassifier with 2 hidden layers and report validation + test metrics including fairness metrics.\n",
    "#     (\n",
    "#         X_train, X_val, X_test,\n",
    "#         y_train, y_val, y_test,\n",
    "#         A_train, A_val, A_test,\n",
    "#         scaler,\n",
    "#     ) = load_dataset(dataset_name, sensitive=sensitive)\n",
    "\n",
    "#     print(f\"MLP baseline | Dataset: {dataset_name}, sensitive: {sensitive}\")\n",
    "#     print(\"Train shape:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n",
    "\n",
    "#     mlp = MLPClassifier(\n",
    "#         hidden_layer_sizes=hidden_layer_sizes,\n",
    "#         activation=\"relu\",\n",
    "#         solver=\"adam\",\n",
    "#         learning_rate_init=0.001,\n",
    "#         max_iter=max_iter,\n",
    "#         random_state=random_state,\n",
    "#     )\n",
    "\n",
    "#     mlp.fit(X_train, y_train)\n",
    "\n",
    "#     # Validation metrics\n",
    "#     y_val_pred = mlp.predict(X_val)\n",
    "#     val_metrics = compute_metrics(y_val, y_val_pred, A_val)\n",
    "\n",
    "#     # Test metrics\n",
    "#     y_test_pred = mlp.predict(X_test)\n",
    "#     test_metrics = compute_metrics(y_test, y_test_pred, A_test)\n",
    "\n",
    "#     print(\"\\n=== MLP Validation metrics ===\")\n",
    "#     for k, v in val_metrics.items():\n",
    "#         if k == \"confusion_matrix\":\n",
    "#             print(k, \"=\\n\", v)\n",
    "#         else:\n",
    "#             print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
    "\n",
    "#     print(\"\\n=== MLP Test metrics ===\")\n",
    "#     for k, v in test_metrics.items():\n",
    "#         if k == \"confusion_matrix\":\n",
    "#             print(k, \"=\\n\", v)\n",
    "#         else:\n",
    "#             print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
    "\n",
    "#     return val_metrics, test_metrics, mlp\n",
    "\n",
    "# # sample MLP runs (can remove later)\n",
    "\n",
    "# mlp_val_taiwan_sex, mlp_test_taiwan_sex, mlp_model_taiwan_sex = run_mlp_baseline(\n",
    "#     dataset_name=\"taiwan\",\n",
    "#     sensitive=\"sex\",\n",
    "#     hidden_layer_sizes=(64, 32),\n",
    "#     max_iter=500,\n",
    "#     random_state=0,\n",
    "# )\n",
    "\n",
    "# mlp_val_taiwan_age, mlp_test_taiwan_age, mlp_model_taiwan_age = run_mlp_baseline(\n",
    "#     dataset_name=\"taiwan\",\n",
    "#     sensitive=\"age\",\n",
    "#     hidden_layer_sizes=(64, 32),\n",
    "#     max_iter=500,\n",
    "#     random_state=0,\n",
    "# )\n",
    "\n",
    "# mlp_val_german_sex, mlp_test_german_sex, mlp_model_german_sex = run_mlp_baseline(\n",
    "#     dataset_name=\"german\",\n",
    "#     sensitive=\"sex\",\n",
    "#     hidden_layer_sizes=(64, 32),\n",
    "#     max_iter=500,\n",
    "#     random_state=0,\n",
    "# )\n",
    "\n",
    "# mlp_val_german_age, mlp_test_german_age, mlp_model_german_age = run_mlp_baseline(\n",
    "#     dataset_name=\"german\",\n",
    "#     sensitive=\"age\",\n",
    "#     hidden_layer_sizes=(64, 32),\n",
    "#     max_iter=500,\n",
    "#     random_state=0,\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) [AI201 Mini Project] Lopez, Alyanna & Mangune, Alexandra v3",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
