{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05868709-fa4f-41f8-bd5f-9212280aee62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, clone\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, confusion_matrix\n",
    "from fairlearn.metrics import demographic_parity_difference, equalized_odds_difference, MetricFrame # for predictive_parity_odds\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "'''\n",
    "============================================================\n",
    " Functions for Data Pre-Processing\n",
    "============================================================\n",
    "'''\n",
    "\n",
    "# DATA_DIR = \"\" # todo: modify this path to match your folder path\n",
    "DATA_DIR = \"/Workspace/Users/alexandra.mangune@gmail.com/Masters/AI201/Mini-Project/\" \n",
    "\n",
    "def load_taiwan_dataset(test_size=0.3, val_size=0.2, random_state=42, sensitive=\"sex\"):\n",
    "    '''\n",
    "    Load the Taiwan dataset (UCI Credit Card).\n",
    "    sensitive: str, \"sex\" or \"age\"\n",
    "    '''\n",
    "    path = os.path.join(DATA_DIR, \"UCI_Credit_Card.csv\")\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.drop(columns=[\"ID\"])\n",
    "\n",
    "    # Target\n",
    "    y = df[\"default.payment.next.month\"].values\n",
    "\n",
    "    # Sensitive attributes\n",
    "    df[\"SEX_BIN\"] = df[\"SEX\"].map({1: 1, 2: 0})\n",
    "    df[\"AGE_NUM\"] = df[\"AGE\"]\n",
    "    df[\"AGE_GROUP\"] = pd.cut(\n",
    "        df[\"AGE\"], bins=[0, 25, 35, 45, 60, 120], labels=[0, 1, 2, 3, 4]\n",
    "    ).astype(int)\n",
    "\n",
    "    if sensitive.lower() == \"sex\":\n",
    "        A = df[\"SEX_BIN\"].values\n",
    "    else:\n",
    "        A = df[\"AGE_GROUP\"].values\n",
    "\n",
    "    df_features = df.drop(columns=[\"default.payment.next.month\"])\n",
    "    X = pd.get_dummies(df_features, drop_first=True)\n",
    "\n",
    "    X_temp, X_test, y_temp, y_test, A_temp, A_test = train_test_split(\n",
    "        X, y, A,\n",
    "        test_size=test_size,\n",
    "        stratify=y,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    val_rel = val_size / (1.0 - test_size)\n",
    "    X_train, X_val, y_train, y_val, A_train, A_val = train_test_split(\n",
    "        X_temp, y_temp, A_temp,\n",
    "        test_size=val_rel,\n",
    "        stratify=y_temp,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    num_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "    X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
    "    X_val[num_cols] = scaler.transform(X_val[num_cols])\n",
    "    X_test[num_cols] = scaler.transform(X_test[num_cols])\n",
    "\n",
    "    return (\n",
    "        X_train, X_val, X_test,\n",
    "        y_train, y_val, y_test,\n",
    "        A_train, A_val, A_test,\n",
    "        scaler,\n",
    "    )\n",
    "\n",
    "def load_german_dataset(test_size=0.3, val_size=0.2, random_state=42, sensitive=\"sex\"):\n",
    "    '''\n",
    "    Load the German dataset (German Credit).\n",
    "    sensitive: str, \"sex\" or \"age\"\n",
    "    '''\n",
    "    path = os.path.join(DATA_DIR, \"Processed_German_Credit.csv\")\n",
    "    df = pd.read_csv(path)\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "    # Target\n",
    "    y = df[\"credit\"].values\n",
    "\n",
    "    df[\"sex_bin\"] = df[\"sex\"].map({\"male\": 1, \"female\": 0})\n",
    "\n",
    "    df[\"age_num\"] = df[\"age\"]\n",
    "    df[\"age_group\"] = pd.cut(\n",
    "        df[\"age\"], bins=[0, 25, 35, 45, 60, 120], labels=[0, 1, 2, 3, 4]\n",
    "    ).astype(int)\n",
    "\n",
    "    if sensitive.lower() == \"sex\":\n",
    "        A = df[\"sex_bin\"].values\n",
    "    else:\n",
    "        A = df[\"age_group\"].values\n",
    "\n",
    "    df_features = df.drop(columns=[\"credit\"])\n",
    "    X = pd.get_dummies(df_features, drop_first=True)\n",
    "\n",
    "    X_temp, X_test, y_temp, y_test, A_temp, A_test = train_test_split(\n",
    "        X, y, A,\n",
    "        test_size=test_size,\n",
    "        stratify=y,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    val_rel = val_size / (1.0 - test_size)\n",
    "    X_train, X_val, y_train, y_val, A_train, A_val = train_test_split(\n",
    "        X_temp, y_temp, A_temp,\n",
    "        test_size=val_rel,\n",
    "        stratify=y_temp,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    num_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "    X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
    "    X_val[num_cols] = scaler.transform(X_val[num_cols])\n",
    "    X_test[num_cols] = scaler.transform(X_test[num_cols])\n",
    "\n",
    "    return (\n",
    "        X_train, X_val, X_test,\n",
    "        y_train, y_val, y_test,\n",
    "        A_train, A_val, A_test,\n",
    "        scaler,\n",
    "    )\n",
    "\n",
    "def load_dataset(name, sensitive=\"sex\", **kwargs):\n",
    "    '''\n",
    "    Call either load_taiwan_dataset() or load_german_dataset() functions to load the dataset.\n",
    "    '''\n",
    "    if name.lower() == 'taiwan':\n",
    "        return load_taiwan_dataset(sensitive=sensitive, **kwargs)\n",
    "    elif name.lower() == 'german':\n",
    "        return load_german_dataset(sensitive=sensitive, **kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {name}\")\n",
    "\n",
    "print(\"\\n--- TAIWAN DATASET CHECK ---\")\n",
    "X_train_t, X_val_t, X_test_t, y_train_t, y_val_t, y_test_t, A_train_t, A_val_t, A_test_t, _ = load_taiwan_dataset()\n",
    "print(f\"Taiwan X_train shape: {X_train_t.shape}\")\n",
    "print(f\"Taiwan X_test shape: {X_test_t.shape}\")\n",
    "print(f\"Taiwan X_val shape: {X_val_t.shape}\")\n",
    "print(f\"Taiwan A_train unique values: {np.unique(A_train_t)}\")\n",
    "\n",
    "print(\"\\n--- GERMAN DATASET CHECK ---\")\n",
    "X_train_g, X_val_g, X_test_g, y_train_g, y_val_g, y_test_g, A_train_g, A_val_g, A_test_g, _ = load_german_dataset(sensitive=\"age\")\n",
    "print(f\"German X_train shape: {X_train_g.shape}\")\n",
    "print(f\"German X_test shape: {X_test_g.shape}\")\n",
    "print(f\"Taiwan X_val shape: {X_val_g.shape}\")\n",
    "print(f\"German A_train unique values: {np.unique(A_train_g)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "523db29b-f2ab-4179-9621-7039742d1b37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "''' \n",
    "============================================================\n",
    " Functions for Computing Evaluation Metrics\n",
    "============================================================\n",
    "'''\n",
    "def compute_metrics(y_true, y_pred, A):\n",
    "    '''\n",
    "    computes evaluation metrics (acc, prec, rec, f1, mcc), confusion matrix, and \n",
    "    fairness metrics (absolute differences of dp, eo, pp).\n",
    "    '''\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "    metric_frame = MetricFrame(\n",
    "        metrics={\"precision\": precision_score},\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        sensitive_features=A\n",
    "    )\n",
    "    group_precision = metric_frame.by_group['precision']\n",
    "    \n",
    "    dp_diff = demographic_parity_difference(y_true=y_true, y_pred=y_pred, sensitive_features=A)\n",
    "    eo_diff = equalized_odds_difference(y_true=y_true, y_pred=y_pred, sensitive_features=A)\n",
    "    pp_diff = abs(group_precision.max() - group_precision.min())\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"mcc\": mcc,\n",
    "        \"dp_diff\": dp_diff,\n",
    "        \"eo_diff\": eo_diff,\n",
    "        \"pp_diff\": pp_diff,\n",
    "        \"confusion_matrix\": confusion_matrix(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "def predictive_parity_difference(y_true, y_pred, sensitive_features):\n",
    "    groups = np.unique(sensitive_features)\n",
    "    ppv = {}\n",
    "\n",
    "    for g in groups:\n",
    "        mask = sensitive_features == g\n",
    "        preds_pos = (y_pred[mask] == 1)\n",
    "        \n",
    "        if preds_pos.sum() == 0:\n",
    "            ppv[g] = 0 \n",
    "        else:\n",
    "            ppv[g] = (y_true[mask][preds_pos] == 1).mean()\n",
    "\n",
    "    gaps = []\n",
    "    for g1 in groups:\n",
    "        for g2 in groups:\n",
    "            gaps.append(abs(ppv[g1] - ppv[g2]))\n",
    "\n",
    "    return max(gaps)\n",
    "\n",
    "def compute_group_error_rates(y_true, y_pred, A):\n",
    "    '''\n",
    "    Solves group-specific False Positive Rate (FPR) and \n",
    "    False Negative Rate (FNR) for each group in A.\n",
    "    Helper function for graphing error rates.\n",
    "    '''\n",
    "    # Assume 0 is negative class, 1 is positive class\n",
    "    groups = np.unique(A)\n",
    "    rates = {}\n",
    "\n",
    "    for group in groups:\n",
    "        mask = A == group\n",
    "        y_true_g = y_true[mask]\n",
    "        y_pred_g = y_pred[mask]\n",
    "        \n",
    "        TN = np.sum((y_true_g == 0) & (y_pred_g == 0))\n",
    "        FP = np.sum((y_true_g == 0) & (y_pred_g == 1))\n",
    "        FN = np.sum((y_true_g == 1) & (y_pred_g == 0))\n",
    "        TP = np.sum((y_true_g == 1) & (y_pred_g == 1))\n",
    "\n",
    "        # False Positive Rate (FPR): FP / (TN + FP)\n",
    "        N_g = TN + FP # True Negatives + False Positives = total actual negatives\n",
    "        FPR_g = FP / N_g if N_g > 0 else 0.0\n",
    "\n",
    "        # False Negative Rate (FNR): FN / (FN + TP)\n",
    "        P_g = FN + TP # False Negatives + True Positives = total actual positives\n",
    "        FNR_g = FN / P_g if P_g > 0 else 0.0\n",
    "\n",
    "        rates[group] = {\"FPR\": FPR_g, \"FNR\": FNR_g}\n",
    "        \n",
    "    return rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "004b9688-e061-4d91-b9f6-9ed23fc37910",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "''' \n",
    "============================================================\n",
    " Functions for Plotting Figures\n",
    "============================================================\n",
    "'''\n",
    "\n",
    "def plot_pareto_tradeoff(val_results, title_suffix=\"\"):\n",
    "    '''\n",
    "    Plot the pareto-style trade-off for accuracy vs all fairness metrics\n",
    "    '''\n",
    "    lambdas = [res[\"lambda\"] for res in val_results]\n",
    "    accs = [res[\"accuracy\"] for res in val_results]\n",
    "    dps = [res[\"dp\"] for res in val_results]\n",
    "    eos = [res[\"eo\"] for res in val_results]\n",
    "    pps = [res[\"pp\"] for res in val_results]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    plt.plot(dps, accs, marker=\"o\", label=\"DP (ΔP)\")\n",
    "    plt.plot(eos, accs, marker=\"s\", label=\"EO (ΔTPR + ΔFPR)\")\n",
    "    plt.plot(pps, accs, marker=\"^\", label=\"PP (ΔPPV)\")\n",
    "\n",
    "    for dp, eo, pp, acc, lam in zip(dps, eos, pps, accs, lambdas):\n",
    "        plt.annotate(f\"{lam}\", (dp, acc), textcoords=\"offset points\", xytext=(5, 5), fontsize=8)\n",
    "        plt.annotate(f\"{lam}\", (eo, acc), textcoords=\"offset points\", xytext=(5, 5), fontsize=8)\n",
    "        plt.annotate(f\"{lam}\", (pp, acc), textcoords=\"offset points\", xytext=(5, 5), fontsize=8)\n",
    "\n",
    "    plt.xlabel(\"Fairness Metric Value (lower = fairer)\")\n",
    "    plt.ylabel(\"Validation Accuracy\")\n",
    "    plt.title(f\"Accuracy vs Fairness Tradeoff {title_suffix}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_error_rate_disparity(group_rates, sensitive_attribute_name, title):\n",
    "    '''\n",
    "    Plots FPR and FNR for each group in a sensitive attribute.\n",
    "    '''\n",
    "    groups = list(group_rates.keys())\n",
    "    fpr_values = [group_rates[g][\"FPR\"] for g in groups]\n",
    "    fnr_values = [group_rates[g][\"FNR\"] for g in groups]\n",
    "    \n",
    "    x = np.arange(len(groups)) \n",
    "    width = 0.35 \n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    rects1 = ax.bar(x - width/2, fpr_values, width, label='FPR (False Positive Rate)', color='skyblue', alpha=0.8)\n",
    "    rects2 = ax.bar(x + width/2, fnr_values, width, label='FNR (False Negative Rate)', color='salmon', alpha=0.8)\n",
    "\n",
    "    ax.set_ylabel('Error Rate')\n",
    "    ax.set_xlabel(f'Sensitive Attribute: {sensitive_attribute_name}')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(groups)\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    def autolabel(rects):\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            ax.annotate(f'{height:.3f}',\n",
    "                        xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                        xytext=(0, 3),  # 3 points vertical offset\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    autolabel(rects1)\n",
    "    autolabel(rects2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_lambda_sensitivity(results, fairness_key=\"dp_diff\", title=\"Lambda Sensitivity\"):\n",
    "    lambdas = [r[\"lambda\"] for r in results]\n",
    "    acc = [r[\"accuracy\"] for r in results]\n",
    "    fairness = [r[fairness_key] for r in results]\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(7,5))\n",
    "    \n",
    "    ax1.plot(lambdas, acc, marker=\"o\", label=\"Accuracy\")\n",
    "    ax1.set_xlabel(\"Lambda\")\n",
    "    ax1.set_ylabel(\"Accuracy\")\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(lambdas, fairness, marker=\"s\", color=\"red\", label=fairness_key.upper())\n",
    "    ax2.set_ylabel(f\"{fairness_key.upper()} Gap\", color=\"red\")\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def plot_training_dynamics(round_metrics, fairness_key=\"dp_diff\", title=\"Boosting Dynamics\"):\n",
    "    rounds = [m[\"round\"] for m in round_metrics]\n",
    "    accs = [m[\"accuracy\"] for m in round_metrics]\n",
    "    fairness_vals = [m[fairness_key] for m in round_metrics]\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "\n",
    "    plt.plot(rounds, accs, marker=\"o\", label=\"Accuracy\")\n",
    "    plt.plot(rounds, fairness_vals, marker=\"s\", label=fairness_key)\n",
    "\n",
    "    plt.xlabel(\"Boosting Round\")\n",
    "    plt.ylabel(\"Metric Value\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_threshold_satisfaction_heatmap(val_results, metrics=(\"accuracy\", \"dp\", \"eo\", \"pp\"), title=\"Threshold Satisfaction Heatmap\"):\n",
    "    model_names = []\n",
    "    for r in val_results:\n",
    "        if r[\"lambda\"] == 0:\n",
    "            model_names.append(\"baseline\")\n",
    "        else:\n",
    "            model_names.append(f\"λ={r['lambda']}\")\n",
    "\n",
    "    data = []\n",
    "    for r in val_results:\n",
    "        row = []\n",
    "        for m in metrics:\n",
    "            row.append(r[m])   \n",
    "        data.append(row)\n",
    "\n",
    "    df = pd.DataFrame(data, columns=metrics, index=model_names)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(df, annot=True, fmt=\".3f\", cmap=\"coolwarm\", linewidths=.5)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Metrics\")\n",
    "    plt.ylabel(\"Models\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a84f9ca8-a21a-455d-8167-532c639fb549",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "''' \n",
    "============================================================\n",
    " AdaBoost implementation\n",
    "============================================================\n",
    "'''\n",
    "class AdaBoostClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, \n",
    "                 base_estimator=None, \n",
    "                 n_estimators=50, \n",
    "                 learning_rate=1.0,\n",
    "                 fairness_lambda=0.0,\n",
    "                 fairness_tolerance=0.5,\n",
    "                 random_state=None):\n",
    "\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.random_state = random_state\n",
    "        self.fairness_lambda = fairness_lambda\n",
    "        self.fairness_tolerance = fairness_tolerance\n",
    "\n",
    "        self.estimators_ = []\n",
    "        self.estimator_weights_ = []\n",
    "        self.classes_ = None\n",
    "\n",
    "        self.round_metrics = []   # store metrics per round\n",
    "        self.training_time_ = None\n",
    "        self.last_predict_time_ = None\n",
    "\n",
    "    def check_binary(self, y):\n",
    "        y = np.asarray(y)\n",
    "        classes = np.unique(y)\n",
    "        if len(classes) != 2:\n",
    "            raise ValueError(\"Binary classification only.\")\n",
    "\n",
    "        self.classes_ = classes\n",
    "        return (y == classes[1]).astype(int)\n",
    "\n",
    "    def fit(self, X, y, A=None):\n",
    "        '''\n",
    "        fit with fairness modification using demographic parity gap.\n",
    "        A is the sensitive attribute (0/1).\n",
    "        '''\n",
    "        if self.base_estimator is None:\n",
    "            raise ValueError(\"base_estimator must be provided.\")\n",
    "\n",
    "        if A is None:\n",
    "            raise ValueError(\"Sensitive attribute A must be provided for fairness-aware training.\")\n",
    "\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        A = np.asarray(A)\n",
    "\n",
    "        y01 = self.check_binary(y)\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        sample_weight = np.ones(n_samples) / n_samples\n",
    "        rng = np.random.RandomState(self.random_state) # random states\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        self.estimators_ = []\n",
    "        self.estimator_weights_ = []\n",
    "\n",
    "        for t in range(self.n_estimators):\n",
    "            est_idx = t % len(self.base_estimator)\n",
    "            est = clone(self.base_estimator[est_idx])\n",
    "            # print(f\"Round {t}: Using estimator {type(est).__name__}\")\n",
    "            \n",
    "            idx = rng.choice(\n",
    "                np.arange(n_samples),\n",
    "                size=n_samples,\n",
    "                replace=True,\n",
    "                p=sample_weight\n",
    "            )\n",
    "            est.fit(X[idx], y01[idx])\n",
    "\n",
    "            y_pred = est.predict(X).astype(int)\n",
    "            incorrect = (y_pred != y01).astype(float)\n",
    "\n",
    "            err = np.dot(sample_weight, incorrect) / sample_weight.sum()\n",
    "            err = np.clip(err, 1e-10, 1 - 1e-10)\n",
    "            acc = accuracy_score(y, y_pred)\n",
    "\n",
    "            lambda_dp = 0.1\n",
    "            lambda_eo = 0.3\n",
    "            lambda_pp = 0.5\n",
    "            \n",
    "            dp_gap = demographic_parity_difference(y_true=y01, y_pred=y_pred, sensitive_features=A)\n",
    "            eo_gap = equalized_odds_difference(y_true=y01, y_pred=y_pred, sensitive_features=A)\n",
    "            pp_gap = predictive_parity_difference(y01, y_pred, A)\n",
    "\n",
    "            dp = np.clip(dp_gap, 0, 1)\n",
    "            eo = np.clip(eo_gap, 0, 1)\n",
    "            pp = np.clip(pp_gap, 0, 1)\n",
    "            \n",
    "            if self.fairness_lambda >= self.fairness_tolerance: # with fairness ON\n",
    "                fairness_penalty = (lambda_dp * dp) + (lambda_eo * eo) + (lambda_pp * pp)\n",
    "                lambda_t = np.exp(-self.fairness_lambda * fairness_penalty)\n",
    "            else: # fairness OFF (baseline AdaBoost)\n",
    "                fairness_penalty = 0.0\n",
    "                lambda_t = 1.0\n",
    "\n",
    "            alpha = 0.5 * np.log((1 - err) / err) * self.learning_rate\n",
    "            if alpha <= 0:\n",
    "                break\n",
    "\n",
    "            sample_weight *= np.exp(-alpha * (2 * y01 - 1) * (2 * y_pred - 1))\n",
    "            sample_weight *= np.exp(-self.fairness_lambda * fairness_penalty)\n",
    "            sample_weight /= sample_weight.sum()\n",
    "\n",
    "            self.estimators_.append(est)\n",
    "            self.estimator_weights_.append(alpha * lambda_t)\n",
    "            self.round_metrics.append({\n",
    "                \"round\": t + 1,\n",
    "                \"accuracy\": acc,\n",
    "                \"dp_diff\": dp,\n",
    "                \"eo_diff\": eo,\n",
    "                \"pp_diff\": pp\n",
    "            })\n",
    "\n",
    "        self.estimator_weights_ = np.array(self.estimator_weights_)\n",
    "        end_time = time.perf_counter()\n",
    "        self.training_time_ = end_time - start_time\n",
    "        return self\n",
    "\n",
    "    def scores(self, X):\n",
    "        X = np.asarray(X)\n",
    "        scores = np.zeros(X.shape[0])\n",
    "\n",
    "        for alpha, est in zip(self.estimator_weights_, self.estimators_):\n",
    "            pred = est.predict(X).astype(int)\n",
    "            pred_pm1 = 2 * pred - 1\n",
    "            scores += alpha * pred_pm1\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def predict(self, X):\n",
    "        start_time = time.perf_counter()\n",
    "        scores = self.scores(X)\n",
    "        y01 = (scores >= 0).astype(int)\n",
    "        end_time = time.perf_counter()\n",
    "        self.last_predict_time_ = end_time - start_time\n",
    "        return np.where(y01 == 1, self.classes_[1], self.classes_[0])\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        scores = self.scores(X)\n",
    "        p_pos = 1 / (1 + np.exp(-2 * scores))\n",
    "        return np.vstack([1 - p_pos, p_pos]).T\n",
    "        \n",
    "def run_experiment(\n",
    "    dataset_name,\n",
    "    sensitive=\"sex\",\n",
    "    lambdas=(0.0, 0.1, 0.3, 0.5, 1.0),\n",
    "    n_estimators=50,\n",
    "    fairness_tolerance=0.05, # for fairness modifications\n",
    "):\n",
    "    '''\n",
    "    runs the experiment for a given dataset and sensitive attribute.\n",
    "    '''\n",
    "    (\n",
    "        X_train, X_val, X_test,\n",
    "        y_train, y_val, y_test,\n",
    "        A_train, A_val, A_test, # this is for the fairness modification\n",
    "        scaler,\n",
    "    ) = load_dataset(dataset_name, sensitive=sensitive)\n",
    "\n",
    "    print(f\"Dataset: {dataset_name}, sensitive: {sensitive}\")\n",
    "    print(\"Train shape:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n",
    "\n",
    "    base = [\n",
    "        Perceptron(max_iter=1000, eta0=0.01, random_state=0, tol=1e-3, fit_intercept=True),\n",
    "        SVC(probability=True, kernel='rbf', C=1.0),\n",
    "        MLPClassifier(\n",
    "            hidden_layer_sizes=(128, 32),\n",
    "            max_iter=500,\n",
    "            early_stopping=True,\n",
    "            n_iter_no_change=20,\n",
    "            validation_fraction=0.1,\n",
    "            random_state=42,\n",
    "            learning_rate_init=0.001,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    val_results = []\n",
    "\n",
    "    for lam in lambdas:\n",
    "        model = AdaBoostClassifier(\n",
    "            base_estimator=base,\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=1.0,\n",
    "            fairness_lambda=lam, # for fairness modifications\n",
    "            fairness_tolerance=fairness_tolerance # for fairness modifications\n",
    "        )\n",
    "        # fairness-aware fit uses A_train\n",
    "        model.fit(X_train, y_train, A_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        metrics = compute_metrics(y_val, y_val_pred, A_val)\n",
    "        metrics[\"lambda\"] = lam\n",
    "        \n",
    "        val_results.append({\n",
    "            \"metrics\": metrics,\n",
    "            \"lambda\": lam,\n",
    "            \"accuracy\": metrics['accuracy'],\n",
    "            \"dp\": metrics['dp_diff'],\n",
    "            \"eo\": metrics['eo_diff'],\n",
    "            \"pp\": metrics['pp_diff'],\n",
    "            \"training_time\": model.training_time_,       \n",
    "            \"validation_time\": model.last_predict_time_,  \n",
    "            \"round_metrics\": model.round_metrics\n",
    "        })\n",
    "\n",
    "        print(\n",
    "            f\"lambda={lam:.2f} | \"\n",
    "            f\"Acc={metrics['accuracy']:.3f}, F1={metrics['f1']:.3f}, \"\n",
    "            f\"MCC={metrics['mcc']:.3f}, DP={metrics['dp_diff']:.3f}, \"\n",
    "    \t    f\"EO={metrics['eo_diff']:.3f}, PP={metrics['pp_diff']:.3f}, \"\n",
    "            f\"Train Time={model.training_time_:.2f}s, \"\n",
    "            f\"Val Time={model.last_predict_time_:.4f}s\" \n",
    "        )\n",
    "\n",
    "    best_idx = np.argmax([metrics[\"f1\"] for m in val_results])\n",
    "    best_lam = val_results[best_idx][\"lambda\"]\n",
    "    print(\"\\nBest lambda on validation (by F1):\", best_lam)\n",
    "\n",
    "    X_tr_full = pd.concat([X_train, X_val], axis=0)\n",
    "    y_tr_full = np.concatenate([y_train, y_val])\n",
    "    A_tr_full = np.concatenate([A_train, A_val])\n",
    "\n",
    "    baseline_lam = 0.0\n",
    "    \n",
    "    baseline_model = AdaBoostClassifier(\n",
    "        base_estimator=base,\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=1.0,\n",
    "        fairness_lambda=baseline_lam,\n",
    "        fairness_tolerance=fairness_tolerance,\n",
    "    )\n",
    "    baseline_model.fit(X_tr_full, y_tr_full, A_tr_full)\n",
    "    y_test_pred_baseline = baseline_model.predict(X_test)\n",
    "    baseline_rates = compute_group_error_rates(y_test, y_test_pred_baseline, A_test)\n",
    "    \n",
    "    best_model = AdaBoostClassifier(\n",
    "        base_estimator=base,\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=1.0,\n",
    "        fairness_lambda=best_lam,\n",
    "        fairness_tolerance=fairness_tolerance,\n",
    "    )\n",
    "    best_model.fit(X_tr_full, y_tr_full, A_tr_full)\n",
    "    y_test_pred_best = best_model.predict(X_test)\n",
    "    best_fair_rates = compute_group_error_rates(y_test, y_test_pred_best, A_test)\n",
    "\n",
    "    test_metrics = compute_metrics(y_test, y_test_pred_best, A_test)\n",
    "    test_metrics[\"training_time\"] = best_model.training_time_\n",
    "    test_metrics[\"testing_time\"] = best_model.last_predict_time_\n",
    "    \n",
    "    print(\"\\n=== Test metrics with best lambda ===\")\n",
    "    for k, v in test_metrics.items():\n",
    "        if k == \"confusion_matrix\":\n",
    "            print(k, \"=\\n\", v)\n",
    "        else:\n",
    "            print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
    "\n",
    "    return val_results, test_metrics, baseline_rates, best_fair_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "296c53ac-5b1d-458a-b90b-1446f9254621",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "''' \n",
    "==================================================================\n",
    " Run AdaBoost experiments (Decision Trees)\n",
    "==================================================================\n",
    "'''\n",
    "\n",
    "def run_decision_tree_experiment(\n",
    "    dataset_name,\n",
    "    sensitive=\"sex\",\n",
    "    lambdas=(0.0, 0.1, 0.3, 0.5, 1.0),\n",
    "    n_estimators=50,\n",
    "    fairness_tolerance=0.05,\n",
    "    max_depth=1 \n",
    "):\n",
    "    '''\n",
    "    Runs the experiment using Decision Trees as the base estimator.\n",
    "    '''\n",
    "    (\n",
    "        X_train, X_val, X_test,\n",
    "        y_train, y_val, y_test,\n",
    "        A_train, A_val, A_test,\n",
    "        scaler,\n",
    "    ) = load_dataset(dataset_name, sensitive=sensitive)\n",
    "\n",
    "    print(f\"Dataset: {dataset_name}, sensitive: {sensitive}\")\n",
    "    print(\"Train shape:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n",
    "\n",
    "    base = [\n",
    "        DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n",
    "    ]\n",
    "\n",
    "    val_results = []\n",
    "\n",
    "    for lam in lambdas:\n",
    "        model = AdaBoostClassifier(\n",
    "            base_estimator=base,\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=1.0,\n",
    "            fairness_lambda=lam,\n",
    "            fairness_tolerance=fairness_tolerance\n",
    "        )\n",
    "        \n",
    "        model.fit(X_train, y_train, A_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        metrics = compute_metrics(y_val, y_val_pred, A_val)\n",
    "        metrics[\"lambda\"] = lam\n",
    "        \n",
    "        val_results.append({\n",
    "            \"metrics\": metrics,\n",
    "            \"lambda\": lam,\n",
    "            \"accuracy\": metrics['accuracy'],\n",
    "            \"dp\": metrics['dp_diff'],\n",
    "            \"eo\": metrics['eo_diff'],\n",
    "            \"pp\": metrics['pp_diff'],\n",
    "            \"f1\": metrics['f1'],\n",
    "            \"mcc\": metrics['mcc'],\n",
    "            \"training_time\": model.training_time_,       \n",
    "            \"validation_time\": model.last_predict_time_,  \n",
    "            \"round_metrics\": model.round_metrics\n",
    "        })\n",
    "\n",
    "        print(\n",
    "            f\"lambda={lam:.2f} | \"\n",
    "            f\"Acc={metrics['accuracy']:.3f}, F1={metrics['f1']:.3f}, \"\n",
    "            f\"MCC={metrics['mcc']:.3f}, DP={metrics['dp_diff']:.3f}, \"\n",
    "            f\"EO={metrics['eo_diff']:.3f}, PP={metrics['pp_diff']:.3f}, \"\n",
    "            f\"Train Time={model.training_time_:.2f}s, \"\n",
    "            f\"Val Time={model.last_predict_time_:.4f}s\" \n",
    "        )\n",
    "\n",
    "    best_idx = np.argmax([res[\"f1\"] for res in val_results])\n",
    "    best_lam = val_results[best_idx][\"lambda\"]\n",
    "    print(\"\\nBest lambda on validation (by F1):\", best_lam)\n",
    "\n",
    "    X_tr_full = pd.concat([X_train, X_val], axis=0)\n",
    "    y_tr_full = np.concatenate([y_train, y_val])\n",
    "    A_tr_full = np.concatenate([A_train, A_val])\n",
    "\n",
    "    baseline_model = AdaBoostClassifier(\n",
    "        base_estimator=base,\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=1.0,\n",
    "        fairness_lambda=0.0,\n",
    "        fairness_tolerance=fairness_tolerance,\n",
    "    )\n",
    "    baseline_model.fit(X_tr_full, y_tr_full, A_tr_full)\n",
    "    y_test_pred_baseline = baseline_model.predict(X_test)\n",
    "    baseline_rates = compute_group_error_rates(y_test, y_test_pred_baseline, A_test)\n",
    "    \n",
    "    best_model = AdaBoostClassifier(\n",
    "        base_estimator=base,\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=1.0,\n",
    "        fairness_lambda=best_lam,\n",
    "        fairness_tolerance=fairness_tolerance,\n",
    "    )\n",
    "    best_model.fit(X_tr_full, y_tr_full, A_tr_full)\n",
    "    y_test_pred_best = best_model.predict(X_test)\n",
    "    best_fair_rates = compute_group_error_rates(y_test, y_test_pred_best, A_test)\n",
    "\n",
    "    test_metrics = compute_metrics(y_test, y_test_pred_best, A_test)\n",
    "    test_metrics[\"training_time\"] = best_model.training_time_\n",
    "    test_metrics[\"testing_time\"] = best_model.last_predict_time_\n",
    "    \n",
    "    print(\"\\n=== Test metrics with best lambda (Decision Tree) ===\")\n",
    "    for k, v in test_metrics.items():\n",
    "        if k == \"confusion_matrix\":\n",
    "            print(k, \"=\\n\", v)\n",
    "        else:\n",
    "            print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
    "\n",
    "    return val_results, test_metrics, baseline_rates, best_fair_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f05ddbb-5831-4e10-9456-a037e939e997",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "''' \n",
    "==================================================================\n",
    " Run AdaBoost experiments (Round-Robin Heterogeneous)\n",
    "==================================================================\n",
    "'''\n",
    "# Taiwan dataset fairness on sex\n",
    "val_res_taiwan_sex, test_taiwan_sex, baseline_taiwan_sex, best_taiwan_sex = run_experiment(\n",
    "    dataset_name=\"taiwan\",\n",
    "    sensitive=\"sex\",\n",
    "    lambdas=(0.0, 0.1, 0.3, 0.5, 0.8, 1.0),\n",
    "    n_estimators=40,\n",
    "    fairness_tolerance=0.5, # for fairness modifications\n",
    ")\n",
    "\n",
    "plot_pareto_tradeoff(val_res_taiwan_sex, title_suffix=\"(Taiwan, Sex)\")\n",
    "plot_lambda_sensitivity(val_res_taiwan_sex, fairness_key=\"dp\", title=\"Lambda DP Sensitivity (Taiwan vs Sex)\") \n",
    "plot_lambda_sensitivity(val_res_taiwan_sex, fairness_key=\"eo\", title=\"Lambda EO Sensitivity (Taiwan vs Sex)\") \n",
    "plot_lambda_sensitivity(val_res_taiwan_sex, fairness_key=\"pp\", title=\"Lambda PP Sensitivity (Taiwan vs Sex)\")\n",
    "\n",
    "f1_scores = [res[\"metrics\"][\"f1\"] for res in val_res_taiwan_sex]\n",
    "best_f1_idx = np.argmax(f1_scores)\n",
    "best_f1_result = val_res_taiwan_sex[best_f1_idx]\n",
    "target_lambda = best_f1_result[\"lambda\"]\n",
    "round_metrics_best_f1 = best_f1_result[\"round_metrics\"]\n",
    "\n",
    "plot_error_rate_disparity(baseline_taiwan_sex, sensitive_attribute_name=\"Sex (0=Female, 1=Male)\", title=f\"Test Error Disparity - Baseline Model ($\\\\lambda=0.0$)\")\n",
    "plot_error_rate_disparity(best_taiwan_sex, sensitive_attribute_name=\"Sex (0=Female, 1=Male)\", title=f\"Test Error Disparity - Fair Model ($\\\\lambda={target_lambda:.2f}$)\")\n",
    "plot_threshold_satisfaction_heatmap(val_res_taiwan_sex, metrics=[\"accuracy\", \"dp\", \"eo\", \"pp\"], title=\"Threshold Satisfaction Heatmap (Taiwan, Sex)\")\n",
    "\n",
    "plot_training_dynamics(round_metrics_best_f1, fairness_key=\"dp_diff\", title=f\"Training Dynamics (λ={target_lambda}) DP\")\n",
    "plot_training_dynamics(round_metrics_best_f1, fairness_key=\"eo_diff\", title=f\"Training Dynamics (λ={target_lambda}) EO\")\n",
    "plot_training_dynamics(round_metrics_best_f1, fairness_key=\"pp_diff\", title=f\"Training Dynamics (λ={target_lambda}) PP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd2c063f-287f-455e-9ae3-1bf62d789358",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Taiwan dataset fairness on age\n",
    "val_res_taiwan_age, test_taiwan_age, baseline_taiwan_age, best_taiwan_age = run_experiment(\n",
    "    dataset_name=\"taiwan\",\n",
    "    sensitive=\"age\",\n",
    "    lambdas=(0.0, 0.1, 0.3, 0.5, 0.8, 1.0),\n",
    "    n_estimators=40,\n",
    "    fairness_tolerance=0.5,\n",
    ")\n",
    "\n",
    "plot_pareto_tradeoff(val_res_taiwan_age, title_suffix=\"(Taiwan, Age)\")\n",
    "plot_lambda_sensitivity(val_res_taiwan_age, fairness_key=\"dp\", title=\"Lambda DP Sensitivity (Taiwan vs Age)\")\n",
    "plot_lambda_sensitivity(val_res_taiwan_age, fairness_key=\"eo\", title=\"Lambda EO Sensitivity (Taiwan vs Age)\")\n",
    "plot_lambda_sensitivity(val_res_taiwan_age, fairness_key=\"pp\", title=\"Lambda PP Sensitivity (Taiwan vs Age)\")\n",
    "\n",
    "f1_scores_age = [res[\"metrics\"][\"f1\"] for res in val_res_taiwan_age]\n",
    "best_f1_idx_age = np.argmax(f1_scores_age)\n",
    "best_f1_result_age = val_res_taiwan_age[best_f1_idx_age]\n",
    "target_lambda_age = best_f1_result_age[\"lambda\"]\n",
    "round_metrics_best_f1_age = best_f1_result_age[\"round_metrics\"]\n",
    "\n",
    "plot_error_rate_disparity(baseline_taiwan_age, sensitive_attribute_name=\"Age Group (0-4)\", title=f\"Test Error Disparity - Baseline Model ($\\\\lambda=0.0$)\")\n",
    "plot_error_rate_disparity(best_taiwan_age, sensitive_attribute_name=\"Age Group (0-4)\", title=f\"Test Error Disparity - Fair Model ($\\\\lambda={target_lambda_age:.2f}$)\")\n",
    "plot_threshold_satisfaction_heatmap(val_res_taiwan_age, metrics=[\"accuracy\", \"dp\", \"eo\", \"pp\"], title=\"Threshold Satisfaction Heatmap (Taiwan, Age)\")\n",
    "\n",
    "plot_training_dynamics(round_metrics_best_f1_age, fairness_key=\"dp_diff\", title=f\"Training Dynamics (λ={target_lambda_age}) DP\")\n",
    "plot_training_dynamics(round_metrics_best_f1_age, fairness_key=\"eo_diff\", title=f\"Training Dynamics (λ={target_lambda_age}) EO\")\n",
    "plot_training_dynamics(round_metrics_best_f1_age, fairness_key=\"pp_diff\", title=f\"Training Dynamics (λ={target_lambda_age}) PP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b284c2dc-f418-46a9-acc0-05b649a88703",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# German dataset fairness on sex\n",
    "val_res_german_sex, test_german_sex, baseline_german_sex, best_german_sex = run_experiment(\n",
    "    dataset_name=\"german\",\n",
    "    sensitive=\"sex\",\n",
    "    lambdas=(0.0, 0.1, 0.3, 0.5, 0.8, 1.0),\n",
    "    n_estimators=40,\n",
    "    fairness_tolerance=0.5,\n",
    ")\n",
    "\n",
    "plot_pareto_tradeoff(val_res_german_sex, title_suffix=\"(German, Sex)\")\n",
    "plot_lambda_sensitivity(val_res_german_sex, fairness_key=\"dp\", title=\"Lambda DP Sensitivity (German vs Sex)\")\n",
    "plot_lambda_sensitivity(val_res_german_sex, fairness_key=\"eo\", title=\"Lambda EO Sensitivity (German vs Sex)\")\n",
    "plot_lambda_sensitivity(val_res_german_sex, fairness_key=\"pp\", title=\"Lambda PP Sensitivity (German vs Sex)\")\n",
    "\n",
    "f1_scores_sex = [res[\"metrics\"][\"f1\"] for res in val_res_german_sex]\n",
    "best_f1_idx_sex = np.argmax(f1_scores_sex)\n",
    "best_f1_result_sex = val_res_german_sex[best_f1_idx_sex]\n",
    "target_lambda_sex = best_f1_result_sex[\"lambda\"]\n",
    "round_metrics_best_f1_sex = best_f1_result_sex[\"round_metrics\"]\n",
    "\n",
    "plot_error_rate_disparity(baseline_german_sex, sensitive_attribute_name=\"Sex (0=Female, 1=Male)\", title=f\"Test Error Disparity - Baseline Model ($\\\\lambda=0.0$)\")\n",
    "plot_error_rate_disparity(best_german_sex, sensitive_attribute_name=\"Sex (0=Female, 1=Male)\", title=f\"Test Error Disparity - Fair Model ($\\\\lambda={target_lambda_sex:.2f}$)\")\n",
    "plot_threshold_satisfaction_heatmap(val_res_german_sex, metrics=[\"accuracy\", \"dp\", \"eo\", \"pp\"], title=\"Threshold Satisfaction Heatmap (German, Sex)\")\n",
    "\n",
    "plot_training_dynamics(round_metrics_best_f1_sex, fairness_key=\"dp_diff\", title=f\"Training Dynamics (λ={target_lambda_sex}) DP\")\n",
    "plot_training_dynamics(round_metrics_best_f1_sex, fairness_key=\"eo_diff\", title=f\"Training Dynamics (λ={target_lambda_sex}) EO\")\n",
    "plot_training_dynamics(round_metrics_best_f1_sex, fairness_key=\"pp_diff\", title=f\"Training Dynamics (λ={target_lambda_sex}) PP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1f3e18b-cd3b-4a0e-8cd7-5fb5124dab27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# German dataset fairness on age\n",
    "val_res_german_age, test_german_age, baseline_german_age, best_german_age = run_experiment(\n",
    "    dataset_name=\"german\",\n",
    "    sensitive=\"age\",\n",
    "    lambdas=(0.0, 0.1, 0.3, 0.5, 0.8, 1.0),\n",
    "    n_estimators=40,\n",
    "    fairness_tolerance=0.5,\n",
    ")\n",
    "\n",
    "plot_pareto_tradeoff(val_res_german_age, title_suffix=\"(German, Age)\")\n",
    "plot_lambda_sensitivity(val_res_german_age, fairness_key=\"dp\", title=\"Lambda DP Sensitivity (German vs Age)\")\n",
    "plot_lambda_sensitivity(val_res_german_age, fairness_key=\"eo\", title=\"Lambda EO Sensitivity (German vs Age)\")\n",
    "plot_lambda_sensitivity(val_res_german_age, fairness_key=\"pp\", title=\"Lambda PP Sensitivity (German vs Age)\")\n",
    "\n",
    "f1_scores_age_g = [res[\"metrics\"][\"f1\"] for res in val_res_german_age]\n",
    "best_f1_idx_age_g = np.argmax(f1_scores_age_g)\n",
    "best_f1_result_age_g = val_res_german_age[best_f1_idx_age_g]\n",
    "target_lambda_age_g = best_f1_result_age_g[\"lambda\"]\n",
    "round_metrics_best_f1_age_g = best_f1_result_age_g[\"round_metrics\"]\n",
    "\n",
    "plot_error_rate_disparity(baseline_german_age, sensitive_attribute_name=\"Age Group (0-4)\", title=f\"Test Error Disparity - Baseline Model ($\\\\lambda=0.0$)\")\n",
    "plot_error_rate_disparity(best_german_age, sensitive_attribute_name=\"Age Group (0-4)\", title=f\"Test Error Disparity - Fair Model ($\\\\lambda={target_lambda_age_g:.2f}$)\")\n",
    "plot_threshold_satisfaction_heatmap(val_res_german_age, metrics=[\"accuracy\", \"dp\", \"eo\", \"pp\"], title=\"Threshold Satisfaction Heatmap (German, Age)\")\n",
    "\n",
    "plot_training_dynamics(round_metrics_best_f1_age_g, fairness_key=\"dp_diff\", title=f\"Training Dynamics (λ={target_lambda_age_g}) DP\")\n",
    "plot_training_dynamics(round_metrics_best_f1_age_g, fairness_key=\"eo_diff\", title=f\"Training Dynamics (λ={target_lambda_age_g}) EO\")\n",
    "plot_training_dynamics(round_metrics_best_f1_age_g, fairness_key=\"pp_diff\", title=f\"Training Dynamics (λ={target_lambda_age_g}) PP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8461d57-f5c5-460a-9e97-cce5ea83b3d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba8ac442-927c-4c89-bd77-f52755b6bacf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "[AI201 Mini Project] Lopez, Alyanna & Mangune, Alexandra v4",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}